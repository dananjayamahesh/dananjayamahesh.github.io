<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Online Predictive Big Data Analysis With Streaming ML Support for WSO2 Machine Learner by dananjayamahesh</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Online Predictive Big Data Analysis With Streaming ML Support for WSO2 Machine Learner</h1>
      <h2 class="project-tagline">Bigdata - Machine Learning - Predictive Analysis - Massive Online Analysis - Spark - Samoa - WSO2 - CEP - Hadoop  Google Summer Of Code - 2016</h2>
    </section>

    <section class="main-content">
      <p>This Blog Contains the summary of the Google Summer of Code 2016 project with WSO2 and also the Link needed for final evaluation is given at last. This blog is organized in the following way.</p>

<hr>

<div>
Overview
Batch-Processing Vs Streaming Learning
Apache Spark &amp; Mini-Batch Learning
Apache SAMOA &amp; Streaming Learning
WSO2 ML (Machine Learner)
WSO2 CEP (Complex Event Processor)
Solution
Architecture 
Implementation
Getting Started
WSO2 Siddhi Query for Streaming ML analysis
GSOC Final Evaluation Links
</div>

<hr>

<h3>
<a id="overview" class="anchor" href="#overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h3>

<p>This project is to implement incremental machine learning algorithms to re-train machine learning models real time extending WSO2 Machine Learner (ML) for predictive big data analysis with the streaming support and WSO2 CEP (Complex Event Processor) extension support which can be deployed distributedly for massive online analysis. This solution provide hybrid and easily adoptable solution for both volume centric and velocity focused big-data analysis realizing the nature and the form of the big data. Therefore anlaysing data real-time with reduced latencies with incremental machine learning algorithms such as GSD (Stochastic Gradient Descent) Optimization and mini-batch processing with the concepts such as data horizon and data obsolences is the primary focus of the initial project. you can find the WSO2 project idea <a href="https://docs.wso2.com/display/GSoC/Project+Proposals+for+2016#ProjectProposalsfor2016-Proposal6:%5BML%5DPredictiveanalyticswithonlinedataforWSO2MachineLearner">here</a> and my project proposal <a href="docs/proposal.pdf">here</a>. </p>

<hr>

<h3>
<a id="batch-processing-vs-streaming-learning" class="anchor" href="#batch-processing-vs-streaming-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Batch-Processing Vs Streaming Learning</h3>

<p>It is paramount imporatant to underestand the nature of the analysis to come up with a soultion for data analysis.Batch-Processing and supported algorithms for retrain ML models are to address the increasing vloume of the data. But with the rapid growth of the Internet of things (IoT) the velocity of the data is becoming vital to reduce the responce time such as in the case of sensor data processing. Mini-Batch processing is the easier way to handle large scale data which can be processed distributedly and independently. Batch Processing is the efficient way to address high volume of data where algorithms such as MapReduce are used to divide massive datasets across multiple distributed clusters for processing. But studies has shown that to extract near real tim insight of massive data, it is much better to use streaming learning. But with the nature if data it is paramount important to handle two scenarios in a hybrid soultion to handle both adoptably.</p>

<hr>

<h3>
<a id="apache-spark-and-mini-batch-processing" class="anchor" href="#apache-spark-and-mini-batch-processing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Apache Spark and Mini-Batch Processing</h3>

<p>Spark has mini-batch processing based incremetal algorithms which is currenlty one of best solution for distributed processing with the fastest MapReduce algorithm than Hadoop. And also Spark has streaming solution which divide massive data sets or data streams into distributed streams and process against incremental algorithms. Apache Spark has a incremental learning algorithm support sudh as mini-batch algorithms, (SGD) stochastic gradient optimizations. Storm is the underlying distributed stream processing framework for spark. Thei streaming solution is advances to their MLLib, which mae use of same set of algorithms upon distributed streams fro streaming analysis. Therefore Spache Spark is ideal for handling high volume of data rather than processing faster which may be the case in extracting near insight of streaming learning.
Most important factor is that even though the Apache Spark support mini-nbatch learning we can extensd its features to train models and retrain them as streams in real time as micro batches by reducing the batch size. This is not encouraged when the massive data volume is the critical factor for the  anlaysis. Because in the batch-processing there is always a additional delay. Batch processing is really good for the situation where we need strategic insight when there is no need to process data in real time. Therefore spark is ideal for mini-batch processing scenarios.</p>

<hr>

<h3>
<a id="apache-samoa-and-streaming-learning" class="anchor" href="#apache-samoa-and-streaming-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Apache SAMOA and Streaming Learning</h3>

<p>SAMOA (Scalable and Massive Online Analysis) is one of the emerging technologies that can be used for streaming analysis in real time. This SAMOA has already solved the scalability issues arised with its predecesors MOA, WEKA etc. This SAMOA is processing streams real time and can do fast retraining in real time analysis. And also SAMOA can be deplyed distributedly and process in a distributed cluster which can be powered by Storm, Samza and S4.With the scalable and flexible architecture SAMOA can support streaming learning with massive data. And also it can support both types of streaming learning.</p>

<ol>
<li>Native Stream Processing (tuple-at-a-time processing).</li>
<li>Micro Batch Processing. </li>
</ol>

<p>Both approaches have it's pros and cons. If we need to do massive analysis and resond real time, the native learning approach is more appropriate. Though this apprach is resulting in lowest possible latency, as long as the data volume is getting bigger it is not computationally efficient.In that case processing can be done as micro-batch processing with relevant ML algorithms. More importantly SAMOA provides basic building blocks wchich can be used for comples streaming analysis topologies which make it more scalable. You can find more information on Apache SAMOA <a href="https://samoa.incubator.apache.org/">here</a>.  </p>

<hr>

<h3>
<a id="wso2-ml-machine-learner" class="anchor" href="#wso2-ml-machine-learner" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>WSO2 ML (Machine Learner)</h3>

<p>WSO2 use <code>carbon-ml</code> as their machine learning (ML) backbone which is based on the famous Apache Spark. With objective of supporting Big Data with a massive online analysis their intial project idea to extend their ML capabilities to real time retrain ML models with time without forgetting past which is the current scenario. Therefore the objective is to retrain with present data and past insight of data (basically the past ML models) and retain them fro predictions with prediction streams. In this way they do not need to store massive amount of datasets to training where model contains the insight of data. Currently WSO2 ML has different extensions to work with their CEP (Complex Event Processor) which is their stream/event-processing solution. For more  information and documentation of WSO2 ML please check <a href="http://wso2.com/products/machine-learner/">here</a>. Ultimate objective of this project is to develop API with a CEP siddhi extension that can be deployed on carbon-ml and as well as standalone siddhi extension.</p>

<hr>

<h3>
<a id="wso2-cep-complex-event-processor" class="anchor" href="#wso2-cep-complex-event-processor" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>WSO2 CEP (Complex Event Processor)</h3>

<p>CEP is their main event/stream processing framework which is built upon Apache Storm for distribution. CEP can handle streams and events in better way so we can perform integration, division etc. Therefore CEP has the potential to act as a stream processing engine which is similar to Apache spark streaming. CEP can deploy CEP siddhi extensions which are very easy to build and integrate with the CEP and siddhi queries. Complete set if instructions and guidance i used to develop CEP siddhi extension for my analysis is build with the help of WSO2 documentations of ML CEP <a href="https://docs.wso2.com/display/ML100/WSO2+CEP+Extension+for+ML+Predictions">extension</a>. And also integration of the extensions with the CEP was done with this <a href="http://sachinij.blogspot.com/2014/12/writing-siddhi-extensions.html">blog</a>. For more information on WSO2 CEP can be found <a href="http://wso2.com/products/complex-event-processor/">here</a>. In my solution i have created three CEP siddhi extensions for my project.</p>

<hr>

<h3>
<a id="solution" class="anchor" href="#solution" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solution</h3>

<p>We have provided the two solutions based on the WSO2 ML team requirement. This two solutions address two different aspects of big real time data analysis. high data volume aware solution which can address streaming learning and purely streaming aware solution which can address the volume by using micro-batch processing. They are spark based and samoa based real time predictive big data analysis. According to the WSO2 project idea the first implementation is based on the Apache Spark mini-batch processing and incremental learning algorithms such as SDG (Stochastic Gradient Descent) Optimization for Streaming Linear Regression, mini-batch clustering for streaming Kmeans Clustering. Both algorithms periodically retrain and update the ML model and both have their own CEP siddhi extension to invoke from CEP side for predictive analysis. But additionally after mid-review of GSOC WSO2 ML is looking for much better streaming solution. Therefore i have do review on SAMOA framework and integrate it with the cep for high end streaming learning. Therefore As the second phase of my project i have to integrate novel and advanced samoa architecture for real time streaming predictive analysis with a CEP siddhi extension support to deply in the CEP side.Therefore in the complete project i have developed three core modules,</p>

<ol>
<li> Apache Spark based Streaming Linear Regression with mini-batch SGD (Stochastic Gradient Descent) Optimization</li>
<li> Apache Spark Based Streaming Kmeans Clustering with mini-batch clustering</li>
<li> Apache Samoa Based Streaming Clustering (Integration of SAMOA with WSO2 CEP)</li>
</ol>

<hr>

<h3>
<a id="architecture" class="anchor" href="#architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Architecture</h3>

<ol>
<li>      Spark Based Real Time Predictive Big Data Analysis</li>
</ol>

<p>Both Streaming Linear Regression with SGD and Streaming Clustering withi mini-batch clustering is based on the same architetcure with different Siddhi extensions and core classes. In both spark cases Streaming classes inside my API handles the mini-batch processing.</p>

<p><img src="images/sparkml.jpg" alt="Spark Based Streaming ML Analysis"></p>

<p>Though the classes are named as Streaming.. they can use for both cases mini-batch learning and streaming learning. Spark based architecture is purely mini-batch learning and samoa based implementation is native streaming learning one. But as long as they have opposite counterparts we can use two implementations wiseversa. We can use saprk based implementation for streaming by reducing batch sizes to micro batches and we can use samoa based architecture to micro-batch learning. In the spark case the learning cycles are based on the minibatches and not as streams which may be good for high volume of data. </p>

<ol>
<li>      Samoa Based Real Time Massive Predictive Analysis</li>
</ol>

<p>Integration of SAMOA (Scalable and Massive Online Analysis) with WSO2 CEP is a challenging task. To do that i had to go through both CEP architecture and SAMOA architecture. In Samoa by using their basic building blocks we can create new predictive analysis ML topologies to train ML models learn from data by extracting insight of near massive data. To integrate SAMOA with the new framework such as WSO2 CEP, we should have a good knowledge to handle samoa blocks. By this way we can feed data coming from the CEP to SAMOA ML topologies built for our purposes. To make use of the SAMOA for custom integration please look at samoa <a href="https://samoa.incubator.apache.org/documentation/Home.html">here</a>. For CEP and our API integration SAMOA, customized SAMOA modules are used which we will discussed in the Implementation. In the SAMOA integration its is paramount important to decide the architecture for core to handle data as streams for real time learning. You can find a custom SMOA Task which can be build with basic SAMOA component in the following.</p>

<p><img src="images/samoaml2.jpg" alt="Samoa Topology"></p>

<p>Overall Architecture of the SAMOA based implementation is looks like below when SAMOA is integrated with CEP extension to feed data streams. Because of th simple and distributed nature of the SAMOA, it is very easy to build complex and massive streaming analysis topologies with SAMOA building blocks. As a initial steps we have build a SAMOA streaming clustering topology as shown in the above figure to analyses stream data on-line. More details of the implementation can be found in Implementation.</p>

<p><img src="images/samoaml.jpg" alt="Samoa Predictive Analysis"></p>

<p>Both implementations anre JAVA based implementations. Samoa is using instances and InstanceStreams not Events and EventStreams like in CEP. Therefore stream data is buffered and convert from CEP events to SAMOA instances for analysis. CEP has their native ExecutionPlans to invoke predictive analysis which is used to build the predictive analysis Task in SAMOA core. Because of this preserving streaming analysis architecture for data streams as it is, model is retrained at every instance.</p>

<h4>
<a id="flexibility-and-adjust-ability" class="anchor" href="#flexibility-and-adjust-ability" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Flexibility and Adjust ability</h4>

<p>We have parameterized both implementations and three scenarios where user can easily specify the type and nature of big data analysis that need. If we take analysis wise we have two: Streaming Linear Regression and Streaming Clustering analysis. We have three scenarios if we look at implementation wise. Both Streaming Linear regression and Steaming Clustering with Spark and Streaming Clustering with SAMOA. If we take analysis wide user can adjust following parameters.</p>

<ul>
<li>Learn Type: Learning method (batch-window, moving-batch-window, time-window)</li>
<li>Window-Shift: Apply only when the moving batch window is applied</li>
<li>Mini-Batch - Size:  How often the ML model get retrained or updated</li>
<li>Number of iteration: Number of iteration runs on the SGD algorithms</li>
<li>Step-Size: related to Step Size of the SGD optimization algorithm.</li>
<li>Mini-Batch Fraction: Fraction of the mini-Batch to process at every iteration in SGD algorithm. </li>
<li>Confidence Interval : This is for future use if there will be any requirement</li>
<li>Variable-List : feature Vector of the Data point </li>
</ul>

<p>For the Streaming Clustering Analysis user can adjust following parameters according to the nature of the analysis.</p>

<ul>
<li>Learn Type: Learning method (batch-window, moving-batch-window, time-window)</li>
<li>Window-Shift: Apply only when the moving batch window is applied</li>
<li>Mini-Batch - Size:  How often the ML model get retrained or updated</li>
<li>Number of iteration: Number of iteration runs on the SGD algorithms</li>
<li>Number of Clusters: Number of Clusters need for us</li>
<li>Alpha : Decay Factor which can be used for Data obsolescence</li>
<li>Confidence Interval : This is for future use if there will be any requirement</li>
<li>Variable-List : feature Vector of the Data</li>
</ul>

<hr>

<h3>
<a id="implementation" class="anchor" href="#implementation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Implementation</h3>

<p>Implementation consist of three main components and three main CEP siddhi StreamProcessor Extensions. You can find the final code base of <a href="https://github.com/dananjayamahesh/streamingml">streamingml</a> <a href="https://github.com/dananjayamahesh/streamingml">here</a>. It is the final trimmed package.</p>

<ol>
<li>StreamingLinearRegression - Streaming Linear Regression with SGD based on Spark</li>
<li>StreamingKMeansClustering - Streaming KMeans Clustering based on Spark</li>
<li>StreamingClustring - Streaming Clustering based on SAMOA</li>
</ol>

<h4>
<a id="dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h4>

<p>As long as i use Maven to build with dependencies i use these following dependencies in my pom.xml .
WSO2 CEP Siddhi Dependencies</p>

<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;org.wso2.siddhi&lt;/groupId&gt;
      &lt;artifactId&gt;siddhi-core&lt;/artifactId&gt;
      &lt;version&gt; 3.0.6-SNAPSHOT &lt;/version&gt;
     &lt;/dependency&gt;
</code></pre>

<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;org.wso2.siddhi&lt;/groupId&gt;
      &lt;artifactId&gt;siddhi-query-api&lt;/artifactId&gt;
      &lt;version&gt; 3.0.6-SNAPSHOT &lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Apache Spark Dependencies</p>

<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
      &lt;version&gt;1.6.1&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-mllib_2.10&lt;/artifactId&gt;
      &lt;version&gt;1.6.1&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Apache SAMOA dependencies</p>

<pre><code>   &lt;dependency&gt;
      &lt;groupId&gt;org.apache.samoa&lt;/groupId&gt;
      &lt;artifactId&gt;samoa-api&lt;/artifactId&gt;
      &lt;version&gt;0.4.0-incubating-SNAPSHOT&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.samoa&lt;/groupId&gt;
      &lt;artifactId&gt;samoa-local&lt;/artifactId&gt;
      &lt;version&gt;0.4.0-incubating-SNAPSHOT&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Samoa dependencies are important. Since latest version of samoa 0.4.0-incubator is not in the maven repository you have two options. Either find remote repository or build samoa latest locally. In my case i use locally built Samoa 0.4.0-incubator. For more information of getting started with SAMOA click <a href="https://samoa.incubator.apache.org/">here</a> . May be this <a href="https://samoa.incubator.apache.org/">link</a> will be helpful too. And also please use the below dependency since it is using in SAMOA.</p>

<pre><code>    &lt;dependency&gt;
      &lt;groupId&gt;com.github.javacliparser&lt;/groupId&gt;
      &lt;artifactId&gt;javacliparser&lt;/artifactId&gt;
      &lt;version&gt;0.5.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<h4>
<a id="wso2-cep-siddhi-extension" class="anchor" href="#wso2-cep-siddhi-extension" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>WSO2 CEP Siddhi Extension</h4>

<p>All three components have their native CEP siddhi extensions based on the StreamProcessor Extension. CEP Extension is a one way of using the core components with outside data streams.These are the three classes extending CEP StreamProcessor.</p>

<pre><code>public class StreamingLinearRegressionStreamProcessor extends StreamProcessor
</code></pre>

<pre><code>public class StreamingKMeansClusteringStreamProcessor extends StreamProcessor
</code></pre>

<pre><code>public class StreamingClusteringWithSamoaStreamProcessor extends StreamProcessor
</code></pre>

<p>And also to publish these extensions on the CEP server we have to follow two steps. We have to add following lines to <code>.siddhiext</code> file in the resources.</p>

<pre><code>streamlinreg=org.wso2.carbon.ml.siddhi.extension.streamingml.StreamingLinearRegressionStreamProcessor
</code></pre>

<pre><code>streamclustering=org.wso2.carbon.ml.siddhi.extension.streamingml.StreamingKMeansClusteringStreamProcessor
</code></pre>

<pre><code>streamclusteringsamoa=org.wso2.carbon.ml.siddhi.extension.streamingml.StreamingClusteringWithSamoaStreamProcessor
</code></pre>

<h4>
<a id="streaming-linear-regression-with-sgd-based-on-spark" class="anchor" href="#streaming-linear-regression-with-sgd-based-on-spark" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Streaming Linear Regression with SGD based on Spark</h4>

<p>Streaming Linear Regression classes are developed in the following way. If anyone want to use it without CEP siddhi extension, you have to intantiate the StreamingLinearRegression Class with correct parameters. Then call <code>regress()</code> function in the class with data points as <code>double[]</code>.</p>

<pre><code>public StreamingLinearRegression(int learnType,int windowShift,int paramCount, int batchSize, double ci, int numIteration, double stepSize, double miniBatchFraction)
</code></pre>

<p>then call the regression function <code>regress()</code> to train and retrain the ML models.</p>

<pre><code>public Object[] regress(Double[] eventData)
</code></pre>

<h4>
<a id="streaming-kmeans-clustering-based-on-spark" class="anchor" href="#streaming-kmeans-clustering-based-on-spark" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Streaming KMeans Clustering based on Spark</h4>

<p>Streaming KMeans Clustering can be used either as a CEP extension or standalone API. you just need to instantiate the StreamingKMeansClustering class with relevant parameters.</p>

<pre><code>StreamingKMeansClustering(int learnType, int windowShift, int numAttributes, int batchSize, double ci, int numClusters, int numIterations, double alpha)
</code></pre>

<p>and also you just need to call the <code>cluster()</code> function with data point or event as <code>double[]</code> to train ML model.</p>

<pre><code>public Object[] cluster(Double[] eventData)
</code></pre>

<h4>
<a id="streaming-clustering-based-on-samoa" class="anchor" href="#streaming-clustering-based-on-samoa" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Streaming Clustering based on SAMOA</h4>

<p>Even though the internal structure and functionality is different than the Spark based streaming clustering solution you can use same call and instantiation for the StreamingClustering with Samoa based implementation.</p>

<pre><code>public StreamingClustering(int learnType,int paramCount, int batchSize, double ci, int numClusters,int numIteration, double alpha){
</code></pre>

<p>to train the model with data/event with the correct instantiation you have to call <code>cluster()</code> function</p>

<pre><code>public Object[] cluster(double[] eventData)
</code></pre>

<h4>
<a id="samoa-topology-for-cep-integration" class="anchor" href="#samoa-topology-for-cep-integration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SAMOA Topology For CEP Integration</h4>

<p>This is the challenging part of this entire project. Understanding SAMOA architecture and integrating it with the CEP event streams through CEP siddhi extension consumed time, but finally paid of with good results. As we discussed above <code>StreamingClustering.java</code> is the core module that integrate CEP extension with my SAMOA ML analysis topology.you can find necessary classes i developed for this at <code>org.wso2.carbon.ml.siddhi.extension.streamingml.samoa</code> . There you can see essential components for this integration for Streaming Clustering solution for CEP. In the same way you can develop your own classes to provide other streaming ml analysis on top of this essentials.These samoa supported classes are implemented in the <code>org.wso2.carbon.ml.siddhi.extension.streamingml.samoa</code></p>

<ol>
<li>Streaming Clustering (StreamingClustering.java)</li>
<li>Streaming Clustering Task Builder (StreamingClusteringTaskBuilder.java)</li>
<li>Streaming Clustering Task (StreamingClusteringTask.java)</li>
<li>Streaming Clustering Stream (StreamingClusteringStream.java)</li>
<li>Streaming Clustering Entrance Processor (StreamingClusteringEntranceProcessor.java)</li>
<li>Streaming Clustering Evaluation Processor (StreamingClusteringEvaluationProcessor.java )</li>
</ol>

<p>Inside Streaming Clustering class there are ingress and egress buffers to buffer the streams and convert them from CEP side to SAMOA side and vise versa. Ingress queue for events or event data while egress queue consist of clustering model results such as number of clusters and cluster centers. This is inside <code>StreamingClustering.java</code></p>

<pre><code> public ConcurrentLinkedQueue&lt;double[]&gt;cepEvents;
 public ConcurrentLinkedQueue&lt;Clustering&gt;samoaClusters ;
</code></pre>

<p>Streaming Clustering Task Builder is the one who invoke the my topology for learning and training. In that task builder it initialize the Task class which contains the information of the topology.When a new task is assigned the Task Builder builds the task with necessary parameters.</p>

<pre><code>task.setFactory(new SimpleComponentFactory());
task.init();
SimpleEngine.submitTopology(task.getTopology());
</code></pre>

<p>When the task is identified as Streaming Clustering Task, it will be instantiated and initialize the ml topology successfully. When we take Streaming Clustering Task which inherited by samoa Task can be easily deployed as regular SAMOA Task at <code>org.apache.samoa.tasks.Task</code>.</p>

<pre><code>public class StreamingClusteringTask implements Task, Configurable
</code></pre>

<p>Main idea of Task is to connect streams with processors and build a complex topology. In this clustering analysis case i connected my Streaming Clustering Stream with my Streaming Clustering Entrance Processor. And Then My entrance processor is connected to SAMOA Learning components sequential way or distributed way. Then the topology output is extracted by my Streaming Clustering Evaluation processor which send output model to egress buffer. When we take <code>StreamingClusteringStream.java</code> it is inherited from Samoa ClusteringStream. Or else we can inherit from samoa InstanceStream. Either way we have to generate InstanceStream which is very similar to CEP Event Streams. Because insode SAMOA every data point is processed as <code>Instance</code>.</p>

<pre><code>public class StreamingClusteringStream  extends ClusteringStream 
</code></pre>

<p>In that <code>StreamingClusteringStream.java</code> we have to override some methods to feed custom streams into Samoa core for predictive analysis. If you want to define your own InstanceStream or custom stream that need to connect to Samoa you can simply override following methods as i did with following inside <code>StreamingClusteringStream.java</code>. </p>

<pre><code>protected void prepareForUseImpl(TaskMonitor taskMonitor, ObjectRepository objectRepository)
public InstancesHeader getHeader() 
public boolean hasMoreInstances()
public Example&lt;Instance&gt; nextInstance()         
</code></pre>

<p>And SAMOA is using ContentEvent to pass  through streams which contains elements what we need to process such as Instance. And to feed my streams into samoa toy have to override the my Entrance Processor: <code>StreamingClusteringEntranceProcessor.java</code> .</p>

<pre><code>public ContentEvent nextEvent()
</code></pre>

<p>Finally we need to build evaluation or result capturing module from learners. Therefore i developed my <code>StreamingClusteringEvaluationProcessor.java</code> module to extract clustering results from Learning modules. In flexible samoa architecture we can define our own <code>Learner</code> modules to train ML models.</p>

<pre><code>public class StreamingClusteringEvaluationProcessor implements Processor
</code></pre>

<p>Inside evaluation processor i have to override its <code>process()</code> method for this integration and get back results.</p>

<pre><code>public boolean process(ContentEvent event) 
</code></pre>

<p>Therefore now complete predictive analysis topology with SAMOA has already built. So now you can build and initialize the task.</p>

<hr>

<h3>
<a id="getting-started" class="anchor" href="#getting-started" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting Started</h3>

<p>If you are supposed to use WSO2 CEP framework to test the <code>streamingml</code> package download it. For more information please refer <a href="http://wso2.com/products/complex-event-processor/">this</a>. If you want to use this as a third party API you dont need WSO2 CEP.follow below steps <code>streamingml</code> package in your project.</p>

<pre><code>git clone https://github.com/dananjayamahesh/streamingml.git
cd streamingml
mvn package
</code></pre>

<p>If you want to use it with CEP please follow the following steps and execute the siddhi queries listed in the next section. Please follow the instruction of WSO2 CEP documentation to create example execution plan.
*Copy target/streamingml-1.0-SNAPSHOT.jar into CEP_HOME/repository/component/lib</p>

<ul>
<li>go to CEP_HOME/bin</li>
<li>start the CEP by ./wso2server.sh</li>
<li>Then create InputStream and OutputStream correctly. More info click <a href="https://docs.wso2.com/display/CEP400/Getting+Started">here</a>.</li>
<li>Then go to Execution Plan and copy paste siddhi queries in the next section.</li>
</ul>

<p>And in advance if you want to pack with the relevant jars for this streamingml package with it, please use the maven shade plugin that is commented in the <code>pom.xml</code> file. currently it is using the compiler plugin. Sometimes when CEP cannot identifies the necessary jars such as samoa it will give exceptions. So in that case you have to pack necessary jars inside streamingml jar file. </p>

<pre><code> &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.8&lt;/source&gt;
          &lt;target&gt;1.8&lt;/target&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;
</code></pre>

<p>Therefore if you want to pack necessary jars inside streamingml jar before you put it into CEP_HOME/repository/component/lib , uncomment <code>maven-shade-plugin</code> and comment the <code>maven-compiler-plugin</code>.When you do these steps now you can see two jars inside target/ folder. put both of them inside CEP_HOME/repository/component/lib folder and start CEP again.</p>

<pre><code>  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
       &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;shade&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;transformers&gt;
                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt;
                  &lt;resource&gt;
                    reference.conf
                  &lt;/resource&gt;
                &lt;/transformer&gt;
              &lt;/transformers&gt;
              &lt;instructions&gt;
                &lt;Import-Package&gt;
                  org.apache.spark.network.*;version="1.6.1";
                &lt;/Import-Package&gt;
              &lt;/instructions&gt;
              &lt;descriptorRefs&gt;
                &lt;descriptorRef&gt;
                  jar-with-dependencies
                &lt;/descriptorRef&gt;
              &lt;/descriptorRefs&gt;
              &lt;source&gt;1.8&lt;/source&gt;
              &lt;target&gt;1.8&lt;/target&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;

</code></pre>

<hr>

<h3>
<a id="wso2-siddhi-queries-for-streaming-analysis" class="anchor" href="#wso2-siddhi-queries-for-streaming-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>WSO2 Siddhi Queries for Streaming Analysis</h3>

<p>Siddhi queries are one way of invoking Streaming ML modules in <code>streamingml</code> package.For this example i have been using CCPP dataset from UCI repository. Change streams and parameters to match your case. There are two types of invoking queries. One for Streaming Linear Regression and other one is Streaming Clustering. They are same and inorder as we discussed in the flexibility and adaptability section. But for your convenience i am interpret those query structure.</p>

<h4>
<a id="streaming-linear-regression-query-structure" class="anchor" href="#streaming-linear-regression-query-structure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Streaming Linear Regression Query Structure</h4>

<blockquote>
<p>streamingml:streamlinreg( [learn-type], [window-shift], [mini-batch-size],[number-of-iteration], [step-size], [mini-batch-fraction], [ci], [Variable List]) </p>
</blockquote>

<h4>
<a id="streaming-clustering-query-structure" class="anchor" href="#streaming-clustering-query-structure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Streaming Clustering Query Structure</h4>

<blockquote>
<p>streamingml:streamclustering( [learn-type], [window-shift], [batch-size], [number-iterations], [number-clusters], [alpha], [confidence-interval], [Variable List])</p>
</blockquote>

<p>If you take Spark base one batch-size can be interpreted as the number of data events that the model will be trained and updated. But if you take the same batch-size parameter in SAMOA you can see that batch-size related to the number data events that the model will be updated to cep side, because at each instance or data point it retrain the model.</p>

<h4>
<a id="siddhi-query-for-streaming-linear-regression-spark-based" class="anchor" href="#siddhi-query-for-streaming-linear-regression-spark-based" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Siddhi Query for Streaming Linear regression (Spark Based)</h4>

<pre><code>@Import('ccppInputStream:1.0.0')
define stream ccppInputStream (PE double, ATV double, V double, AP double, RH double);

@Export('ccppOutputStream:1.0.0')
define stream ccppOutputStream (stderr double);

from ccppInputStream#streamingml:streamlinreg(0, 0, 1000,10, 0.00000001, 1.0, 0.95, PE, ATV, V, AP, RH)
select stderr
insert into ccppOutputStream;
</code></pre>

<p>If you want to get model parameters change the output stream according to extract them. OutputStream consist of model parameters.So you can use siddhi query like below where beta0 is the intercept of the streaming regression model.</p>

<pre><code>@Import('ccppInputStream:1.0.0')
define stream ccppInputStream (PE double, ATV double, V double, AP double, RH double);

@Export('ccppOutputStream:1.0.0')
define stream ccppOutputStream (PE double, ATV double, V double, AP double, RH double, stderr double, beta0 double, beta1 double, beta2 double, beta3 double, beta4 double);

from ccppInputStream#streamingml:streamlinreg(0, 0, 1000,10, 0.00000001, 1.0, 0.95, PE, ATV, V, AP, RH)
select *
insert into ccppOutputStream;
</code></pre>

<h4>
<a id="siddhi-query-for-streaming-kmeans-clustering-spark-based" class="anchor" href="#siddhi-query-for-streaming-kmeans-clustering-spark-based" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Siddhi Query for Streaming KMeans Clustering (Spark Based)</h4>

<pre><code>@Import('ccppInputStream:1.0.0')
define stream ccppInputStream (PE double, ATV double, V double, AP double, RH double);

@Export('ccppOutputStream:1.0.0')
define stream ccppOutputStream (PE double, ATV double, V double, AP double, RH double, stderr double, center0 string, center1 string);

from ccppInputStream#streamingml:streamclustering(0, 0, 1000, 10, 2, 1, 0.95, PE, ATV, V, AP, RH)
select *
insert into ccppOutputStream;
</code></pre>

<h4>
<a id="siddhi-query-for-streaming-clustering-samoa-based" class="anchor" href="#siddhi-query-for-streaming-clustering-samoa-based" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Siddhi Query for Streaming Clustering (SAMOA Based)</h4>

<pre><code>@Import('ccppInputStream:1.0.0')
define stream ccppInputStream (PE double, ATV double, V double, AP double, RH double);

@Export('ccppOutputStream:1.0.0')
define stream ccppOutputStream (PE double, ATV double, V double, AP double, RH double, stderr double, center0 string, center1 string);

from ccppInputStream#streamingml:streamclusteringsamoa(0, 0, 1000, 10, 2, 1, 0.95, PE, ATV, V, AP, RH)
select *
insert into ccppOutputStream
</code></pre>

<p>Likewise you can make use of siddhi query to change the analysis and parameter. Lets say you need 3 clusters and you want to get 3 cluster centers back, then you have to use like below,</p>

<pre><code>@Import('ccppInputStream:1.0.0')
define stream ccppInputStream (PE double, ATV double, V double, AP double, RH double);

@Export('ccppOutputStream:1.0.0')
define stream ccppOutputStream (PE double, ATV double, V double, AP double, RH double, stderr double, center0 string, center1 string, center2 string);

from ccppInputStream#streamingml:streamclusteringsamoa(0, 0, 1000, 10, 3, 1, 0.95, PE, ATV, V, AP, RH)
select *
insert into ccppOutputStream
</code></pre>

<p>You can change batch-size and see how the model will be updated with the data streams.</p>

<hr>

<h3>
<a id="gsoc-final-evaluation-links" class="anchor" href="#gsoc-final-evaluation-links" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GSOC Final Evaluation Links</h3>

<ol>
<li><p>GSOC My Work <a href="https://github.com/dananjayamahesh/GSOC2016">here</a> <a href="https://github.com/dananjayamahesh/GSOC2016">https://github.com/dananjayamahesh/GSOC2016</a>).you can find samoa extensions and spark siddhi extension inside <code>gsoc/</code> folder insode GSOC2016 repo. This contains all the major commits i have done during the GSOC period.</p></li>
<li><p>streamingml github repository <a href="https://github.com/dananjayamahesh/streamingml">here</a> (<a href="https://github.com/dananjayamahesh/streamingml">https://github.com/dananjayamahesh/streamingml</a>)</p></li>
<li><p>streamingml integration with carbon-ml master branch <a href="https://github.com/dananjayamahesh/carbon-ml/tree/master/components/extensions/org.wso2.carbon.ml.siddhi.extension/src/main/java/org/wso2/carbon/ml/siddhi/extension">here</a> (<a href="https://github.com/dananjayamahesh/carbon-ml/tree/master/components/extensions/org.wso2.carbon.ml.siddhi.extension/src/main/java/org/wso2/carbon/ml/siddhi/extension">https://github.com/dananjayamahesh/carbon-ml/tree/master/components/extensions/org.wso2.carbon.ml.siddhi.extension/src/main/java/org/wso2/carbon/ml/siddhi/extension</a>)</p></li>
<li><p>carbon-ml PR <a href="https://github.com/wso2/carbon-ml/pull/232">here</a> (<a href="https://github.com/wso2/carbon-ml/pull/232">https://github.com/wso2/carbon-ml/pull/232</a>)</p></li>
<li>
<p>Documentation <a href="docs/documentation.pdf">here</a></p>

<hr>
</li>
</ol>

<h3>
<a id="author" class="anchor" href="#author" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Author</h3>

<p>Mahesh Dananjaya (<a href="https://github.com/dananjayamahesh" class="user-mention">@dananjayamahesh</a>)</p>

<h3>
<a id="contact-us" class="anchor" href="#contact-us" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact Us</h3>

<p>Mahesh Dananjaya (<a href="mailto:dananjayamahesh@gmail.com">dananjayamahesh@gmail.com</a>)</p>

      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
